{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70225573-6a63-47b5-9e4e-3311014fe228",
   "metadata": {},
   "source": [
    "# 1. PCA for Network Intrusion Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05100ad6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:44:11.395800Z",
     "start_time": "2025-05-27T13:44:11.389617Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_kdd_data(data_dir=\"./datasets/kdd_balanced\"):\n",
    "    \"\"\"\n",
    "    Load the balanced dataset from Parquet format (ultra-fast loading).\n",
    "    \n",
    "    Returns:\n",
    "        D_balanced: Feature matrix (numpy array)\n",
    "        is_normal_balanced: Boolean labels (numpy array)  \n",
    "        original_indices: Original indices from full dataset (numpy array)\n",
    "        df_balanced: Original dataframe subset (if available)\n",
    "        metadata: Dataset metadata\n",
    "    \"\"\"\n",
    "    from pathlib import Path\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import json\n",
    "    \n",
    "    data_path = Path(data_dir)\n",
    "    \n",
    "    # Check if required files exist\n",
    "    required_files = [\"balanced_dataset.parquet\", \"metadata.json\"]\n",
    "    missing_files = [f for f in required_files if not (data_path / f).exists()]\n",
    "    if missing_files:\n",
    "        raise FileNotFoundError(f\"Required files not found in {data_path}: {missing_files}\")\n",
    "    \n",
    "    # Load main dataset (this is very fast with Parquet)\n",
    "    df_main = pd.read_parquet(data_path / \"balanced_dataset.parquet\")\n",
    "    \n",
    "    # Extract components\n",
    "    feature_cols = [col for col in df_main.columns if col.startswith('feature_')]\n",
    "    D_balanced = df_main[feature_cols].values\n",
    "    is_normal_balanced = df_main['is_normal'].values\n",
    "    original_indices = df_main['original_index'].values\n",
    "    \n",
    "    # Load original dataframe if available\n",
    "    df_balanced = None\n",
    "    if (data_path / \"original_data_balanced.parquet\").exists():\n",
    "        df_balanced = pd.read_parquet(data_path / \"original_data_balanced.parquet\")\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(data_path / \"metadata.json\", 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"Balanced dataset loaded from {data_path} (Parquet format)\")\n",
    "    print(f\"  Features: {D_balanced.shape}\")\n",
    "    print(f\"  Normal: {metadata['n_normal']:,}, Intrusion: {metadata['n_intrusion']:,}\")\n",
    "    \n",
    "    return D_balanced, is_normal_balanced, original_indices, df_balanced, metadata\n",
    "\n",
    "def compute_pca_from_intrusion_data(D_intrusion, r):\n",
    "    \"\"\"\n",
    "    Compute PCA using SVD on intrusion data with rank r.\n",
    "    \n",
    "    Key insight: We train PCA on INTRUSION data, so normal data\n",
    "    will have higher reconstruction error in this learned space.\n",
    "    \n",
    "    Returns:\n",
    "        X: Principal components (learned from intrusion data)\n",
    "        mu_intrusion: Mean vector of intrusion data\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def project_to_low_dimensional_space(D, X, mu_intrusion):\n",
    "    \"\"\"\n",
    "    Project any data points into the low-dimensional space defined by\n",
    "    intrusion-trained PCA components.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def reconstruct_from_low_dimensional(Y, X, mu_intrusion):\n",
    "    \"\"\"Reconstruct data points from low-dimensional coordinates.\"\"\"\n",
    "\n",
    "\n",
    "def compute_reconstruction_error(original, reconstructed):\n",
    "    \"\"\"Compute L2 reconstruction error.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "65fa2795e898f5d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T13:44:18.586799Z",
     "start_time": "2025-05-27T13:44:11.405201Z"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Required files not found in datasets\\kdd_balanced: ['balanced_dataset.parquet', 'metadata.json']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m D_balanced, is_normal_balanced, original_indices, _, metadata \u001b[38;5;241m=\u001b[39m \u001b[43mload_kdd_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 28\u001b[0m, in \u001b[0;36mload_kdd_data\u001b[1;34m(data_dir)\u001b[0m\n\u001b[0;32m     26\u001b[0m missing_files \u001b[38;5;241m=\u001b[39m [f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m required_files \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (data_path \u001b[38;5;241m/\u001b[39m f)\u001b[38;5;241m.\u001b[39mexists()]\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing_files:\n\u001b[1;32m---> 28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRequired files not found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing_files\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Load main dataset (this is very fast with Parquet)\u001b[39;00m\n\u001b[0;32m     31\u001b[0m df_main \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_parquet(data_path \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_dataset.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Required files not found in datasets\\kdd_balanced: ['balanced_dataset.parquet', 'metadata.json']"
     ]
    }
   ],
   "source": [
    "D_balanced, is_normal_balanced, original_indices, _, metadata = load_kdd_data()\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2489cc87",
   "metadata": {},
   "source": [
    "# 2. K-means Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d77a95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 iterations\n",
      "Mean approximation error: 0.1314\n",
      "Normalized Mutual Information: 0.7419\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "def load_iris_data(path=Path(\"datasets/iris/iris.csv\")):\n",
    "    df = pd.read_csv(path)\n",
    "    data = df.iloc[:, :-1].values\n",
    "    labels = df['target'].values\n",
    "    return data, labels\n",
    "\n",
    "def print_iris_info(data, labels):\n",
    "    n_samples, n_features = data.shape\n",
    "    n_classes = len(np.unique(labels))\n",
    "    feature_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "\n",
    "    print(f\"Number of samples: {n_samples}\")\n",
    "    print(f\"Number of features: {n_features}\")\n",
    "    print(f\"Number of classes: {n_classes}\")\n",
    "    print(f\"Feature names: {feature_names}\")\n",
    "    print(f\"Class distribution:\")\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    for cls, count in zip(unique, counts):\n",
    "        print(f\"  Class {cls}: {count} samples\")\n",
    "\n",
    "def init_centroids_greedy_pp(D,r,l=10):\n",
    "    '''\n",
    "        :param r: (int) number of centroids (clusters)\n",
    "        :param D: (np-array) the data matrix\n",
    "        :param l: (int) number of centroid candidates in each step\n",
    "        :return: (np-array) 'X' the selected centroids from the dataset\n",
    "    '''   \n",
    "    rng =  np.random.default_rng(seed=RANDOM_SEED) # use this random generator to sample the candidates (sampling according to given probabilities can be done via rng.choice(..))\n",
    "    n,d = D.shape\n",
    "\n",
    "    # Your code here\n",
    "\n",
    "    indexes = rng.integers(low=0, high=n, size=r)\n",
    "    X = np.array(D[indexes,:]).T\n",
    "    return X\n",
    "\n",
    "# K-means implementation from the lecture slides\n",
    "def RSS(D,X,Y):\n",
    "    return np.sum((D- Y@X.T)**2)\n",
    "    \n",
    "def getY(labels):\n",
    "    Y = np.eye(max(labels)+1)[labels]\n",
    "    return Y\n",
    "    \n",
    "def update_centroid(D,Y):\n",
    "    cluster_sizes = np.diag(Y.T@Y).copy()\n",
    "    cluster_sizes[cluster_sizes==0]=1\n",
    "    return D.T@Y/cluster_sizes\n",
    "    \n",
    "def update_assignment(D,X):\n",
    "    dist = np.sum((np.expand_dims(D,2) - X)**2,1)\n",
    "    labels = np.argmin(dist,1)\n",
    "    return getY(labels)\n",
    "    \n",
    "def kmeans(D,r, X_init, epsilon=0.00001, t_max=10000):\n",
    "    X = X_init.copy()\n",
    "    Y = update_assignment(D,X)\n",
    "    rss_old = RSS(D,X,Y) +2*epsilon\n",
    "    t=0\n",
    "    \n",
    "    #Looping as long as difference of objective function values is larger than epsilon\n",
    "    while rss_old - RSS(D,X,Y) > epsilon and t < t_max-1:\n",
    "        rss_old = RSS(D,X,Y)\n",
    "        X = update_centroid(D,Y)\n",
    "        Y = update_assignment(D,X)\n",
    "        t+=1\n",
    "    print(t,\"iterations\")\n",
    "    return X,Y\n",
    "\n",
    "\n",
    "data, labels = load_iris_data()\n",
    "\n",
    "# Your code here\n",
    "def mean_approx_error(D, X, Y,n,d):\n",
    "    rss = RSS(D,X,Y)\n",
    "    return rss / (n * d)\n",
    "    \n",
    "y = getY(labels)\n",
    "X_init = init_centroids_greedy_pp(data, r=3, l=10)\n",
    "X_final, Y_final = kmeans(data, r=3, X_init=X_init)\n",
    "n = data.shape[0]\n",
    "d = data.shape[1]\n",
    "mean_error = mean_approx_error(data, X_final, Y_final, n, d)\n",
    "nmi = normalized_mutual_info_score(labels, np.argmax(Y_final, axis=1))\n",
    "print(f\"Mean approximation error: {mean_error:.4f}\")\n",
    "print(f\"Normalized Mutual Information: {nmi:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755b0be9",
   "metadata": {},
   "source": [
    "# 3. Netflix Recommender System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8999edd9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T12:59:37.201807Z",
     "start_time": "2025-05-21T12:59:36.986301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   userId  movieId  rating  timestamp\n",
      "0       1        1     4.0  964982703\n",
      "1       1        3     4.0  964981247\n",
      "2       1        6     4.0  964982224\n",
      "3       1       47     5.0  964983815\n",
      "4       1       50     5.0  964982931\n",
      "   movieId                               title  \\\n",
      "0        1                    Toy Story (1995)   \n",
      "1        2                      Jumanji (1995)   \n",
      "2        3             Grumpier Old Men (1995)   \n",
      "3        4            Waiting to Exhale (1995)   \n",
      "4        5  Father of the Bride Part II (1995)   \n",
      "\n",
      "                                        genres  \n",
      "0  Adventure|Animation|Children|Comedy|Fantasy  \n",
      "1                   Adventure|Children|Fantasy  \n",
      "2                               Comedy|Romance  \n",
      "3                         Comedy|Drama|Romance  \n",
      "4                                       Comedy  \n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def load_ratings_data_pandas(data_dir=\"ml-latest-small/\"):\n",
    "    \"\"\"Load data using pandas dataframes.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    assert data_dir.exists(), f\"{data_dir} does not exist\"\n",
    "    \n",
    "    return pd.read_csv(data_dir / 'ratings.csv',sep=',')\n",
    "test1 = load_ratings_data_pandas(data_dir=\"datasets/ml-latest-small/\")\n",
    "print (test1.head())\n",
    "\n",
    "def load_movies_data_pandas(data_dir=\"ml-latest-small/\"):\n",
    "    \"\"\"Load data using pandas dataframes.\"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    assert data_dir.exists(), f\"{data_dir} does not exist\"\n",
    "    return pd.read_csv(data_dir / 'movies.csv')\n",
    "\n",
    "test2 = load_movies_data_pandas(data_dir=\"datasets/ml-latest-small/\")\n",
    "print(test2.head())\n",
    "\n",
    "def filter_data(ratings_data: pd.DataFrame, movies_data: pd.DataFrame):\n",
    "    \"\"\"Filter data. Too little ratings prevent effective use of matrix completion.\"\"\"\n",
    "    ratings_data = ratings_data.pivot(\n",
    "        index='userId',\n",
    "        columns='movieId',\n",
    "        values='rating'\n",
    "    ).fillna(0)\n",
    "\n",
    "    keep_movie = (ratings_data != 0).sum(axis=0) > 100\n",
    "    ratings_data = ratings_data.loc[:, keep_movie]\n",
    "\n",
    "    # Filter movies_data by movieId (columns of ratings_data after filtering)\n",
    "    movies_data = movies_data[movies_data['movieId'].isin(ratings_data.columns)]\n",
    "\n",
    "    keep_user = (ratings_data != 0).sum(axis=1) >= 5\n",
    "    ratings_data = ratings_data.loc[keep_user, :]\n",
    "\n",
    "    return ratings_data, movies_data\n",
    "\n",
    "\n",
    "def print_data_summary(ratings: pd.DataFrame):\n",
    "    n_users = ratings.shape[0]\n",
    "    n_movies = ratings.shape[1]\n",
    "    n_ratings = (ratings != 0).sum().sum()\n",
    "    density = n_ratings / (n_users * n_movies)\n",
    "\n",
    "    print(f\"Dataset Summary\")\n",
    "    print(f\"----------------\")\n",
    "    print(f\"Users: {n_users}\")\n",
    "    print(f\"Movies: {n_movies}\")\n",
    "    print(f\"Total Ratings: {n_ratings}\")\n",
    "    print(f\"Data Density: {density:.4f} (fraction of observed ratings)\")\n",
    "\n",
    "\n",
    "def load_ratings_data(data_dir=\"ml-latest-small/\", print_summary=False):\n",
    "    \"\"\"Load data in numpy format.\"\"\"\n",
    "    ratings, movies = filter_data(\n",
    "        load_ratings_data_pandas(data_dir=data_dir),\n",
    "        load_movies_data_pandas(data_dir=data_dir)\n",
    "    )\n",
    "    if print_summary:\n",
    "        print_data_summary(ratings)\n",
    "    return ratings.to_numpy()\n",
    "\n",
    "\n",
    "def matrix_completion(D, n_features, n_movies, n_users, t_max=100, lambd = 0.1):\n",
    "    np.random.seed(0)\n",
    "    X = np.random.normal(size=(n_movies, n_features))\n",
    "    Y = np.random.normal(size=(n_users, n_features))\n",
    "    O = (D != 0).astype(float)\n",
    "    for t in range(t_max):\n",
    "        #update X\n",
    "        for k in range(n_movies):\n",
    "            O_k = O[:, k]\n",
    "            Y_ok = Y[O_k == 1]\n",
    "            D_k = D[:, k][O_k == 1]\n",
    "            \n",
    "            A = Y_ok.T @ Y_ok + lambd * np.eye(n_features)\n",
    "            B = D_k. T @ Y_ok\n",
    "            X[k] = np.linalg.solve(A, B)\n",
    "\n",
    "        #update Y\n",
    "        for i in range(n_users):\n",
    "            O_i = O[i, :]\n",
    "            X_oi = X[O_i == 1]\n",
    "            D_i = D[i, :][O_i == 1]\n",
    "           \n",
    "            A = X_oi.T @ X_oi + lambd * np.eye(n_features)\n",
    "            B = D_i.T @ X_oi\n",
    "            Y[i] = np.linalg.solve(A, B)\n",
    "    return X, Y\n",
    "\n",
    "def compute_reconstruction_error(D, X, Y):\n",
    "    \"\"\"Compute the reconstruction error.\"\"\"\n",
    "    O = (D != 0).astype(float)\n",
    "    #print(O.shape, X.shape, Y.shape)\n",
    "    D_hat = (Y @ X.T)\n",
    "    #print(D_hat.shape, D.shape)\n",
    "    #D= D * O  # Mask the original data with observed ratings\n",
    "    #D_hat = D_hat * O  # Mask the reconstructed data with observed ratings\n",
    "    error = np.sum(O *(D - D_hat)** 2) / np.sum(O)\n",
    "    return error  # Normalize by number of observed ratings\n",
    "\n",
    "def estimate_ratings(X, Y, movie_names):\n",
    "    \"\"\"Estimate ratings using the learned matrices.\"\"\"\n",
    "    # Find the index of the movie\n",
    "    ratings, movies_list = filter_data(\n",
    "        load_ratings_data_pandas(data_dir=\"datasets/ml-latest-small/\"),\n",
    "        load_movies_data_pandas(data_dir=\"datasets/ml-latest-small/\")\n",
    "    )\n",
    "    movie_list = movies_list['title'].tolist()\n",
    "\n",
    "    movie_index = [movie_list.index(movie) for movie in movie_names if movie in movie_list]\n",
    "    print(f\"Movie indices: {movie_index}\")\n",
    "    movie_ratings = np.ones(len(movie_index))\n",
    "    D_est = Y @ X.T\n",
    "    for i in range(len(movie_index)):\n",
    "        index = np.array(movie_index[i]).astype(int)  # Get the first index (should be unique)\n",
    "        movie_ratings[i] = D_est[0, index]\n",
    "        print(movie_ratings[i], movie_names[i])\n",
    "\n",
    "\n",
    "    #for index in movie_index:\n",
    "    return movie_ratings\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c3e83f2b82e3dccf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-21T12:59:37.485146Z",
     "start_time": "2025-05-21T12:59:37.387487Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary\n",
      "----------------\n",
      "Users: 556\n",
      "Movies: 134\n",
      "Total Ratings: 19694\n",
      "Data Density: 0.2643 (fraction of observed ratings)\n",
      "Reconstruction error: 0.1017\n",
      "Movie indices: [1, 101, 93, 64]\n",
      "3.467955758064519 Jumanji (1995)\n",
      "5.087225256924929 Fight Club (1999)\n",
      "4.734903377216696 Matrix, The (1999)\n",
      "5.206847016232376 Monty Python and the Holy Grail (1975)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([3.46795576, 5.08722526, 4.73490338, 5.20684702])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings = load_ratings_data(\"datasets/ml-latest-small\", print_summary=True)\n",
    "#ratings2 = load_ratings_data(\"datasets/ml-latest-small\", print_summary=False)\n",
    "x, y = matrix_completion(\n",
    "    ratings,\n",
    "    n_features=20,\n",
    "    n_movies=ratings.shape[1],\n",
    "    n_users=ratings.shape[0],\n",
    "    t_max=100,\n",
    "    lambd=0.5\n",
    ")\n",
    "\n",
    "reconstruction_error = compute_reconstruction_error(ratings, x, y)\n",
    "\n",
    "print(f\"Reconstruction error: {reconstruction_error:.4f}\")\n",
    "#movie_names = \"Jumanji (1995)\" # Example movie names\n",
    "movie_names = [\"Jumanji (1995)\", \"Fight Club (1999)\", \"Matrix, The (1999)\", \"Monty Python and the Holy Grail (1975)\" ] # Example movie name\n",
    "estimate_ratings(x, y, movie_names)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81460c82",
   "metadata": {},
   "source": [
    "# 4. Image Classification With Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d56288",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "CNN Embedding Space Visualization\n",
    "\n",
    "This educational module demonstrates:\n",
    "- ResNet-style architecture with skip connections\n",
    "- Embedding space learning for visualization\n",
    "- Domain transfer between MNIST and Fashion-MNIST\n",
    "- Decision boundary visualization\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Tuple, Optional, Callable\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    \"\"\"Configuration parameters for the model and training.\"\"\"\n",
    "\n",
    "    # Model architecture\n",
    "    embedding_dim: int = 2\n",
    "    num_classes: int = 10\n",
    "\n",
    "    # Training hyperparameters\n",
    "    learning_rate: float = 0.9 \n",
    "    momentum: float = 0.9\n",
    "    weight_decay: float = 5e-4\n",
    "    batch_size: int = 128\n",
    "    epochs: int = 5\n",
    "    dropout_rate_1: float = 0.9\n",
    "    dropout_rate_2: float = 0.9\n",
    "\n",
    "    # Visualization\n",
    "    viz_samples: int = 100\n",
    "    viz_zoom: float = 0.7\n",
    "    grid_resolution: float = 0.1 \n",
    "    \n",
    "    # Paths\n",
    "    checkpoint_dir: Path = Path(\"checkpoint\")\n",
    "    model_filename: str = \"embedding_model.pth\"\n",
    "\n",
    "    @property\n",
    "    def device(self) -> str:\n",
    "        \"\"\"Get the appropriate device for computation.\"\"\"\n",
    "        return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Residual block with skip connections and grouped convolutions.\n",
    "\n",
    "    Implements: output = input + F(input)\n",
    "    where F is a residual function composed of BatchNorm→ReLU→Conv layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_channels: int, out_channels: int, kernel_size: int, groups: int = 1):\n",
    "        super().__init__()\n",
    "\n",
    "        groups = min(groups, min(in_channels, out_channels))\n",
    "\n",
    "        # Main convolution pathway\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size,\n",
    "                              padding=\"same\", groups=groups)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size,\n",
    "                              padding=\"same\", groups=min(groups, out_channels))\n",
    "\n",
    "        # Skip connection (identity or dimension adjustment)\n",
    "        self.skip_connection = (\n",
    "            nn.Identity() if in_channels == out_channels\n",
    "            else nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=\"same\")\n",
    "        )\n",
    "\n",
    "        # Pre-activation normalization layers\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.norm2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass implementing residual connection.\"\"\"\n",
    "        identity = self.skip_connection(x)\n",
    "\n",
    "        # Residual pathway: BatchNorm → ReLU → Conv → BatchNorm → ReLU → Conv\n",
    "        out = self.conv1(self.relu(self.norm1(x)))\n",
    "        out = self.conv2(self.relu(self.norm2(out)))\n",
    "\n",
    "        return identity + out\n",
    "\n",
    "\n",
    "class EmbeddingNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    CNN that maps input images to low-dimensional embedding space.\n",
    "\n",
    "    Uses global average pooling instead of flattening to reduce overfitting\n",
    "    and make the model robust to different input sizes.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, dropout_rate_1: float, dropout_rate_2: float):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initial feature extraction\n",
    "        self.initial_conv = nn.Conv2d(1, 32, kernel_size=5, padding=\"same\")\n",
    "        self.initial_norm = nn.BatchNorm2d(32)\n",
    "\n",
    "        # First residual block set (32 channels, groups=2)\n",
    "        self.res_block1 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "        self.res_block2 = ResidualBlock(32, 32, kernel_size=3, groups=2)\n",
    "\n",
    "        # Spatial downsampling\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.norm_after_pool = nn.BatchNorm2d(32)\n",
    "\n",
    "        # Second residual block set (64 channels, groups=4)\n",
    "        self.res_block3 = ResidualBlock(32, 64, kernel_size=3, groups=4)\n",
    "        self.res_block4 = ResidualBlock(64, 64, kernel_size=3, groups=4)\n",
    "\n",
    "        # Final processing\n",
    "        self.final_norm = nn.BatchNorm1d(64)\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, embedding_dim)\n",
    "\n",
    "        # Regularization\n",
    "        self.dropout1 = nn.Dropout(dropout_rate_1)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate_2)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass mapping images to embedding space.\n",
    "\n",
    "        Args:\n",
    "            x: Input tensor of shape (batch_size, 1, 28, 28)\n",
    "\n",
    "        Returns:\n",
    "            Embedding tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        out = F.relu(self.initial_norm(self.initial_conv(x)))\n",
    "        \n",
    "        # print(out.shape)\n",
    "        # out.shape = ? (tensor shape 1)\n",
    "\n",
    "        # First round of residual blocks\n",
    "        out = self.res_block2(self.res_block1(out))\n",
    "        #print(out.shape)\n",
    "\n",
    "        # out.shape = ? (tensor shape 2)\n",
    "        #print(out.shape)\n",
    "\n",
    "        # Pooling\n",
    "        out = self.norm_after_pool(self.pool(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 3)\n",
    "        #print(out.shape)\n",
    "\n",
    "        # Second round of residual blocks\n",
    "        out = self.res_block4(self.res_block3(out))\n",
    "\n",
    "        # out.shape = ? (tensor shape 4)\n",
    "        #print(out.shape)x\n",
    "\n",
    "        # Global average pooling\n",
    "        out = torch.mean(out, dim=(-1, -2))\n",
    "        out = self.final_norm(out)\n",
    "\n",
    "        # out.shape = ? (tensor shape 5)\n",
    "\n",
    "        # Map to embedding space\n",
    "        out = self.dropout1(out)\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = self.dropout2(out)\n",
    "        out = self.fc2(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class EmbeddingClassifier(nn.Module):\n",
    "    \"\"\"Complete model combining embedding network with classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, embedding_dim: int, num_classes: int, config: Config):\n",
    "        super().__init__()\n",
    "        self.embedding_net = EmbeddingNetwork(\n",
    "            embedding_dim, config.dropout_rate_1, config.dropout_rate_2\n",
    "        )\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes, bias=True)\n",
    "\n",
    "        nn.init.normal_(self.classifier.weight, 0, 0.01)\n",
    "        nn.init.constant_(self.classifier.bias, 0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward pass for training and evaluation.\"\"\"\n",
    "        embeddings = self.embedding_net(x)\n",
    "        return self.classifier(embeddings)\n",
    "\n",
    "    def get_embeddings(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Extract embeddings for visualization.\"\"\"\n",
    "        return self.embedding_net(x)\n",
    "\n",
    "    def get_probabilities(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Get class probabilities for confidence visualization.\"\"\"\n",
    "        embeddings = self.embedding_net(x)\n",
    "        return F.softmax(self.classifier(embeddings), dim=1)\n",
    "\n",
    "\n",
    "def create_data_loaders(dataset_class, config: Config) -> Tuple[DataLoader, DataLoader]:\n",
    "    \"\"\"\n",
    "    Create training and test data loaders.\n",
    "\n",
    "    Args:\n",
    "        dataset_class: torchvision dataset class (MNIST or FashionMNIST)\n",
    "        config: Configuration object\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (train_loader, test_loader)\n",
    "    \"\"\"\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))  # MNIST standard values\n",
    "    ])\n",
    "\n",
    "    # Training data\n",
    "    train_dataset = dataset_class(root='./data', train=True, download=True, transform=transform)\n",
    "    if config.num_classes < 10:\n",
    "        mask = train_dataset.targets < config.num_classes\n",
    "        train_dataset.targets = train_dataset.targets[mask]\n",
    "        train_dataset.data = train_dataset.data[mask]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2\n",
    "    )\n",
    "\n",
    "    # Test data\n",
    "    test_dataset = dataset_class(root='./data', train=False, download=True, transform=transform)\n",
    "    if config.num_classes < 10:\n",
    "        mask = test_dataset.targets < config.num_classes\n",
    "        test_dataset.targets = test_dataset.targets[mask]\n",
    "        test_dataset.data = test_dataset.data[mask]\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2\n",
    "    )\n",
    "\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EpochMetrics:\n",
    "    \"\"\"Container for epoch training/evaluation metrics.\"\"\"\n",
    "    accuracy: float\n",
    "    avg_confidence: float\n",
    "    avg_loss: float\n",
    "    total_samples: int\n",
    "    elapsed_time: Optional[float] = None\n",
    "\n",
    "\n",
    "def compute_batch_metrics(logits: torch.Tensor, targets: torch.Tensor, loss: torch.Tensor) -> Tuple[int, float, int]:\n",
    "    \"\"\"\n",
    "    Compute metrics for a single batch.\n",
    "\n",
    "    Args:\n",
    "        logits: Model output logits\n",
    "        targets: Ground truth labels\n",
    "        loss: Computed loss for the batch\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (correct_predictions, total_confidence, batch_size)\n",
    "    \"\"\"\n",
    "    probabilities = F.softmax(logits, dim=1)\n",
    "    confidences, predictions = probabilities.max(1)\n",
    "\n",
    "    correct_predictions = predictions.eq(targets).sum().item()\n",
    "    total_confidence = confidences.sum().item()\n",
    "    batch_size = targets.size(0)\n",
    "\n",
    "    return correct_predictions, total_confidence, batch_size\n",
    "\n",
    "\n",
    "def run_epoch(\n",
    "    model: nn.Module,\n",
    "    criterion,\n",
    "    data_loader: DataLoader,\n",
    "    device: str,\n",
    "    optimizer=None,\n",
    "    is_training: bool = True\n",
    ") -> EpochMetrics:\n",
    "    \"\"\"\n",
    "    Run one epoch of training or evaluation.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        criterion: Loss function\n",
    "        data_loader: Data loader\n",
    "        device: Device to run on\n",
    "        optimizer: Optimizer (required if is_training=True)\n",
    "        is_training: Whether to run in training mode\n",
    "\n",
    "    Returns:\n",
    "        EpochMetrics containing all computed metrics\n",
    "    \"\"\"\n",
    "    if is_training:\n",
    "        if optimizer is None:\n",
    "            raise ValueError(\"Optimizer required for training mode\")\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_confidence = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    start_time = time.time()\n",
    "    context_manager = torch.no_grad() if not is_training else torch.enable_grad()\n",
    "\n",
    "    with context_manager:\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            if is_training:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            if torch.isnan(loss):\n",
    "                print(\"Warning: NaN loss detected\")\n",
    "\n",
    "            if is_training:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Compute batch metrics (always without gradients for metrics)\n",
    "            with torch.no_grad():\n",
    "                batch_correct, batch_confidence, batch_size = compute_batch_metrics(logits, targets, loss)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                total_correct += batch_correct\n",
    "                total_confidence += batch_confidence\n",
    "                total_samples += batch_size\n",
    "\n",
    "    # Calculate final metrics\n",
    "    if total_samples == 0:\n",
    "        print(\"Warning: No samples processed\")\n",
    "        return EpochMetrics(0, 0, float('inf'), 0, time.time() - start_time)\n",
    "\n",
    "    accuracy = 100.0 * total_correct / total_samples\n",
    "    avg_confidence = 100.0 * total_confidence / total_samples\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    elapsed_time = time.time() - start_time\n",
    "\n",
    "    return EpochMetrics(\n",
    "        accuracy=accuracy,\n",
    "        avg_confidence=avg_confidence,\n",
    "        avg_loss=avg_loss,\n",
    "        total_samples=total_samples,\n",
    "        elapsed_time=elapsed_time\n",
    "    )\n",
    "\n",
    "\n",
    "def train_epoch(model: nn.Module, criterion, optimizer, data_loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Train model for one epoch.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (accuracy, average_confidence)\n",
    "    \"\"\"\n",
    "    metrics = run_epoch(model, criterion, data_loader, device, optimizer, is_training=True)\n",
    "\n",
    "    print(f'Train - Loss: {metrics.avg_loss:.3f} | '\n",
    "          f'Acc: {metrics.accuracy:.3f}% ({int(metrics.accuracy * metrics.total_samples / 100)}/{metrics.total_samples}) | '\n",
    "          f'Conf: {metrics.avg_confidence:.2f}% | Time: {metrics.elapsed_time:.2f}s')\n",
    "\n",
    "    return metrics.accuracy, metrics.avg_confidence\n",
    "\n",
    "\n",
    "def evaluate_model(model: nn.Module, criterion, data_loader: DataLoader, device: str) -> Tuple[float, float]:\n",
    "    \"\"\"\n",
    "    Evaluate model on test data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple of (accuracy, average_confidence)\n",
    "    \"\"\"\n",
    "    metrics = run_epoch(model, criterion, data_loader, device, optimizer=None, is_training=False)\n",
    "\n",
    "    print(f'Test  - Loss: {metrics.avg_loss:.3f} | '\n",
    "          f'Acc: {metrics.accuracy:.3f}% ({int(metrics.accuracy * metrics.total_samples / 100)}/{metrics.total_samples}) | '\n",
    "          f'Conf: {metrics.avg_confidence:.2f}%')\n",
    "\n",
    "    return metrics.accuracy, metrics.avg_confidence\n",
    "\n",
    "\n",
    "def save_model(model: nn.Module, accuracy: float, config: Config) -> None:\n",
    "    \"\"\"Save model checkpoint.\"\"\"\n",
    "    config.checkpoint_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    checkpoint = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'accuracy': accuracy,\n",
    "        'config': {\n",
    "            'embedding_dim': config.embedding_dim,\n",
    "            'num_classes': config.num_classes,\n",
    "            'dropout_rate_1': config.dropout_rate_1,\n",
    "            'dropout_rate_2': config.dropout_rate_2,\n",
    "        }\n",
    "    }\n",
    "\n",
    "    save_path = config.checkpoint_dir / config.model_filename\n",
    "    torch.save(checkpoint, save_path)\n",
    "    print(f\"Model saved to {save_path}\")\n",
    "\n",
    "\n",
    "def load_model(config: Config) -> EmbeddingClassifier:\n",
    "    \"\"\"Load model from checkpoint.\"\"\"\n",
    "    load_path = config.checkpoint_dir / config.model_filename\n",
    "\n",
    "    model = EmbeddingClassifier(config.embedding_dim, config.num_classes, config)\n",
    "    checkpoint = torch.load(load_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "\n",
    "    print(f\"Model loaded from {load_path}\")\n",
    "    print(f\"Loaded model accuracy: {checkpoint['accuracy']:.2f}%\")\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_decision_boundary(\n",
    "    model: EmbeddingClassifier,\n",
    "    bounds: Tuple[float, float, float, float],\n",
    "    config: Config,\n",
    "    show_classes: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot decision boundary or confidence map in embedding space.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        bounds: (x_min, x_max, y_min, y_max) for plot region\n",
    "        config: Configuration object\n",
    "        show_classes: If True, show class assignments; if False, show confidence\n",
    "    \"\"\"\n",
    "    x_min, x_max, y_min, y_max = bounds\n",
    "\n",
    "    if not all(np.isfinite([x_min, x_max, y_min, y_max])):\n",
    "        print(\"Warning: Invalid bounds detected, using default range\")\n",
    "        x_min, x_max, y_min, y_max = -10, 10, -10, 10\n",
    "\n",
    "    if x_max <= x_min:\n",
    "        x_max = x_min + 10\n",
    "    if y_max <= y_min:\n",
    "        y_max = y_min + 10\n",
    "\n",
    "    x = np.arange(x_min, x_max, config.grid_resolution, dtype=np.float32)\n",
    "    y = np.arange(y_min, y_max, config.grid_resolution, dtype=np.float32)\n",
    "\n",
    "    if len(x) == 0 or len(y) == 0:\n",
    "        print(\"Warning: Empty grid, adjusting resolution\")\n",
    "        x = np.linspace(x_min, x_max, 50, dtype=np.float32)\n",
    "        y = np.linspace(y_min, y_max, 50, dtype=np.float32)\n",
    "\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "\n",
    "    # Create grid points for evaluation\n",
    "    grid_points = torch.from_numpy(\n",
    "        np.array([xx.ravel(), yy.ravel()]).T\n",
    "    ).float().to(config.device)\n",
    "\n",
    "    # Get model predictions\n",
    "    with torch.no_grad():\n",
    "        probabilities = torch.softmax(model.classifier(grid_points), dim=1)\n",
    "        probabilities = probabilities.cpu().numpy()\n",
    "\n",
    "    # Reshape for contour plotting\n",
    "    if show_classes:\n",
    "        class_assignments = probabilities.argmax(axis=1).reshape(xx.shape)\n",
    "        plt.contourf(xx, yy, class_assignments, levels=config.num_classes, cmap='tab10', alpha=0.7)\n",
    "        plt.colorbar(label='Predicted Class')\n",
    "    else:\n",
    "        confidence_map = probabilities.max(axis=1).reshape(xx.shape)\n",
    "        contour = plt.contourf(xx, yy, confidence_map, levels=20, cmap='viridis', alpha=0.7)\n",
    "        plt.clim(0, 1)\n",
    "        plt.colorbar(contour, label='Max Confidence')\n",
    "\n",
    "    plt.axis('equal')\n",
    "\n",
    "\n",
    "def scatter_images_on_embeddings(\n",
    "    images: torch.Tensor,\n",
    "    embeddings: torch.Tensor,\n",
    "    config: Config\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Scatter actual images at their embedding coordinates.\n",
    "\n",
    "    Args:\n",
    "        images: Input images tensor\n",
    "        embeddings: Corresponding embedding coordinates\n",
    "        config: Configuration object\n",
    "    \"\"\"\n",
    "    num_samples = min(images.shape[0], config.viz_samples)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image = images[i].squeeze().cpu().numpy()\n",
    "        embedding_pos = (embeddings[i, 0].item(), embeddings[i, 1].item())\n",
    "\n",
    "        if not all(np.isfinite(embedding_pos)):\n",
    "            continue\n",
    "\n",
    "        offset_image = OffsetImage(image, cmap=\"gray\", zoom=config.viz_zoom)\n",
    "        annotation_box = AnnotationBbox(\n",
    "            offset_image, embedding_pos, xycoords='data', frameon=False, alpha=0.7\n",
    "        )\n",
    "        plt.gca().add_artist(annotation_box)\n",
    "\n",
    "\n",
    "def visualize_embedding_space(\n",
    "    model: EmbeddingClassifier,\n",
    "    data_loader: DataLoader,\n",
    "    config: Config,\n",
    "    title: str = \"Embedding Space Visualization\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Create comprehensive visualization of embedding space.\n",
    "\n",
    "    Args:\n",
    "        model: Trained model\n",
    "        data_loader: Data loader for visualization\n",
    "        config: Configuration object\n",
    "        title: Plot title\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Get batch of data and embeddings\n",
    "    inputs, _ = next(iter(data_loader))\n",
    "    inputs = inputs.to(config.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embeddings = model.get_embeddings(inputs).cpu()\n",
    "\n",
    "    valid_embeddings = embeddings[torch.isfinite(embeddings).all(dim=1)]\n",
    "\n",
    "    if len(valid_embeddings) == 0:\n",
    "        print(\"Warning: No valid embeddings found, using default bounds\")\n",
    "        bounds = (-10, 10, -10, 10)\n",
    "    else:\n",
    "        margin = 3\n",
    "        x_vals = valid_embeddings[:, 0]\n",
    "        y_vals = valid_embeddings[:, 1]\n",
    "\n",
    "        bounds = (\n",
    "            float(x_vals.min() - margin),\n",
    "            float(x_vals.max() + margin),\n",
    "            float(y_vals.min() - margin),\n",
    "            float(y_vals.max() + margin)\n",
    "        )\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plot_decision_boundary(model, bounds, config)\n",
    "    scatter_images_on_embeddings(inputs.cpu(), embeddings, config)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Embedding Dimension 1')\n",
    "    plt.ylabel('Embedding Dimension 2')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90146b-527a-4670-aa59-61a16bf28273",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main training and evaluation pipeline.\"\"\"\n",
    "config = Config(\n",
    "    epochs= 20,\n",
    "    learning_rate = 0.05,\n",
    "    momentum = 0.8,\n",
    "    weight_decay = 4e-5,\n",
    "    embedding_dim=2,\n",
    "    batch_size = 128,\n",
    "    dropout_rate_1=0.5,\n",
    "    dropout_rate_2=0.5\n",
    ")\n",
    "\n",
    "print(\"CNN Embedding Space Learning\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Using device: {config.device}\")\n",
    "\n",
    "# Create data loaders\n",
    "print(\"\\nPreparing MNIST data...\")\n",
    "mnist_train_loader, mnist_test_loader = create_data_loaders(datasets.MNIST, config)\n",
    "\n",
    "# Create and setup model\n",
    "print(\"Building model...\")\n",
    "model = EmbeddingClassifier(config.embedding_dim, config.num_classes, config)\n",
    "model = model.to(config.device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(\n",
    "    model.parameters(),\n",
    "    lr=config.learning_rate,\n",
    "    momentum=config.momentum,\n",
    "    weight_decay=config.weight_decay\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "print(\"\\nTraining...\")\n",
    "best_accuracy = 0.0\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    print(f'\\nEpoch {epoch + 1}/{config.epochs}:')\n",
    "    train_epoch(model, criterion, optimizer, mnist_train_loader, config.device)\n",
    "    test_acc, _ = evaluate_model(model, criterion, mnist_test_loader, config.device)\n",
    "\n",
    "    if test_acc > best_accuracy:\n",
    "        best_accuracy = test_acc\n",
    "\n",
    "# Save model\n",
    "save_model(model, best_accuracy, config)\n",
    "\n",
    "# Visualize MNIST embeddings\n",
    "print(\"\\nVisualizing MNIST embeddings...\")\n",
    "try:\n",
    "    visualize_embedding_space(model, mnist_test_loader, config, \"MNIST Embedding Space\")\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "\n",
    "# Domain transfer experiment\n",
    "print(\"\\nTesting domain transfer with Fashion-MNIST...\")\n",
    "fashion_train_loader, fashion_test_loader = create_data_loaders(datasets.FashionMNIST, config)\n",
    "\n",
    "fashion_acc, _ = evaluate_model(model, criterion, fashion_test_loader, config.device)\n",
    "\n",
    "# Visualize Fashion-MNIST embeddings\n",
    "try:\n",
    "    visualize_embedding_space(\n",
    "        model, fashion_test_loader, config,\n",
    "        \"Fashion-MNIST Embeddings (MNIST-trained Model)\"\n",
    "    )\n",
    "    plt.show()\n",
    "except Exception as e:\n",
    "    print(f\"Visualization error: {e}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nResults Summary:\")\n",
    "print(f\"MNIST Test Accuracy: {best_accuracy:.2f}%\")\n",
    "print(f\"Fashion-MNIST Accuracy: {fashion_acc:.2f}%\")\n",
    "print(f\"Domain Transfer Gap: {best_accuracy - fashion_acc:.2f}%\")\n",
    "\n",
    "# Reporting the learned weight matrix W:\n",
    "print(\"\\nLearned weight matrix W (classifier):\\n\")\n",
    "\n",
    "# Sanity check\n",
    "if hasattr(model, 'classifier') and isinstance(model.classifier, nn.Linear):\n",
    "    # Extract the weight matrix W from the classifier\n",
    "    # Extract -> Detach -> CPU -> Numpy\n",
    "    W = model.classifier.weight.detach().cpu().numpy()\n",
    "    \n",
    "    print(\"W Shape:\" W.shape)\n",
    "    print(\"Rows below represent the 2D vectors responding to specific digit classes 0-9:\\n\")\n",
    "    \n",
    "    for i, w_i in enumerate(W):\n",
    "        print(f\"  - Weight vector for Digit {i}: [ {w_i[0]:>8.4f}, {w_i[1]:>8.4f} ]\")\n",
    "else:\n",
    "    print(\"No classified layer found!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
